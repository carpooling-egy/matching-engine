{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69eee49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "import uuid\n",
    "import random\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RIDER_DETOUR_PROBS = [0.3, 0.25, 0.2, 0.1, 0.1, 0.05]\n",
    "DRIVER_DETOUR_PROBS = [0.3, 0.25, 0.2, 0.1, 0.1, 0.05]\n",
    "RIDER_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f298e1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(865247, 23)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/mariam/Downloads/Datasets/New folder/Taxi Trips Chicago 2024/Taxi_Trips_-_2024_20240408.csv\")\n",
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47fdb5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trip ID</th>\n",
       "      <th>Taxi ID</th>\n",
       "      <th>Trip Start Timestamp</th>\n",
       "      <th>Trip End Timestamp</th>\n",
       "      <th>Trip Seconds</th>\n",
       "      <th>Trip Miles</th>\n",
       "      <th>Pickup Census Tract</th>\n",
       "      <th>Dropoff Census Tract</th>\n",
       "      <th>Pickup Community Area</th>\n",
       "      <th>Dropoff Community Area</th>\n",
       "      <th>...</th>\n",
       "      <th>Extras</th>\n",
       "      <th>Trip Total</th>\n",
       "      <th>Payment Type</th>\n",
       "      <th>Company</th>\n",
       "      <th>Pickup Centroid Latitude</th>\n",
       "      <th>Pickup Centroid Longitude</th>\n",
       "      <th>Pickup Centroid Location</th>\n",
       "      <th>Dropoff Centroid Latitude</th>\n",
       "      <th>Dropoff Centroid Longitude</th>\n",
       "      <th>Dropoff Centroid  Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0287f53fedcde6b0af9aab9e32cbd2cadb337eaa</td>\n",
       "      <td>e54db25f18193a08f1f5754515e8c338480e04fb938ed3...</td>\n",
       "      <td>03/01/2024 12:00:00 AM</td>\n",
       "      <td>03/01/2024 12:00:00 AM</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>38.75</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>City Service</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1f0034299b914fd7b45002568576a353c83acb2f</td>\n",
       "      <td>4a263f78f56815be4acd38658af8fc1824ce4e15f7ec81...</td>\n",
       "      <td>03/01/2024 12:00:00 AM</td>\n",
       "      <td>03/01/2024 12:15:00 AM</td>\n",
       "      <td>900.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.901207</td>\n",
       "      <td>-87.676356</td>\n",
       "      <td>POINT (-87.6763559892 41.9012069941)</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01a0b77b722a0b91f45cb4fc90b3e64f76fd0681</td>\n",
       "      <td>4f1e94982e6851725add382f7981d64006ae7c38f3664f...</td>\n",
       "      <td>03/01/2024 12:00:00 AM</td>\n",
       "      <td>03/01/2024 12:00:00 AM</td>\n",
       "      <td>711.0</td>\n",
       "      <td>5.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.70</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>City Service</td>\n",
       "      <td>41.792592</td>\n",
       "      <td>-87.769615</td>\n",
       "      <td>POINT (-87.7696154528 41.7925923603)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14aff2071ac81c6450a8f8b0e1534497da900197</td>\n",
       "      <td>de8e8659ceb9eb0da842a46b60c0a5207098ac69fc23ab...</td>\n",
       "      <td>03/01/2024 12:00:00 AM</td>\n",
       "      <td>03/01/2024 12:30:00 AM</td>\n",
       "      <td>1770.0</td>\n",
       "      <td>13.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>47.10</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Sun Taxi</td>\n",
       "      <td>41.980264</td>\n",
       "      <td>-87.913625</td>\n",
       "      <td>POINT (-87.913624596 41.9802643146)</td>\n",
       "      <td>41.965812</td>\n",
       "      <td>-87.655879</td>\n",
       "      <td>POINT (-87.6558787862 41.96581197)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16d3a633e2f380cee4a7fa4677db163b6e91255d</td>\n",
       "      <td>d79d3e19a1d5f6cb0cd4449d5579412262a2f1b182799d...</td>\n",
       "      <td>03/01/2024 12:00:00 AM</td>\n",
       "      <td>03/01/2024 12:15:00 AM</td>\n",
       "      <td>849.0</td>\n",
       "      <td>6.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.82</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>5 Star Taxi</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "      <td>41.965812</td>\n",
       "      <td>-87.655879</td>\n",
       "      <td>POINT (-87.6558787862 41.96581197)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Trip ID  \\\n",
       "0  0287f53fedcde6b0af9aab9e32cbd2cadb337eaa   \n",
       "1  1f0034299b914fd7b45002568576a353c83acb2f   \n",
       "2  01a0b77b722a0b91f45cb4fc90b3e64f76fd0681   \n",
       "3  14aff2071ac81c6450a8f8b0e1534497da900197   \n",
       "4  16d3a633e2f380cee4a7fa4677db163b6e91255d   \n",
       "\n",
       "                                             Taxi ID    Trip Start Timestamp  \\\n",
       "0  e54db25f18193a08f1f5754515e8c338480e04fb938ed3...  03/01/2024 12:00:00 AM   \n",
       "1  4a263f78f56815be4acd38658af8fc1824ce4e15f7ec81...  03/01/2024 12:00:00 AM   \n",
       "2  4f1e94982e6851725add382f7981d64006ae7c38f3664f...  03/01/2024 12:00:00 AM   \n",
       "3  de8e8659ceb9eb0da842a46b60c0a5207098ac69fc23ab...  03/01/2024 12:00:00 AM   \n",
       "4  d79d3e19a1d5f6cb0cd4449d5579412262a2f1b182799d...  03/01/2024 12:00:00 AM   \n",
       "\n",
       "       Trip End Timestamp  Trip Seconds  Trip Miles  Pickup Census Tract  \\\n",
       "0  03/01/2024 12:00:00 AM          15.0        0.09                  NaN   \n",
       "1  03/01/2024 12:15:00 AM         900.0        3.00                  NaN   \n",
       "2  03/01/2024 12:00:00 AM         711.0        5.84                  NaN   \n",
       "3  03/01/2024 12:30:00 AM        1770.0       13.36                  NaN   \n",
       "4  03/01/2024 12:15:00 AM         849.0        6.13                  NaN   \n",
       "\n",
       "   Dropoff Census Tract  Pickup Community Area  Dropoff Community Area  ...  \\\n",
       "0                   NaN                    8.0                     8.0  ...   \n",
       "1                   NaN                   24.0                     8.0  ...   \n",
       "2                   NaN                   56.0                     NaN  ...   \n",
       "3                   NaN                   76.0                     3.0  ...   \n",
       "4                   NaN                    8.0                     3.0  ...   \n",
       "\n",
       "   Extras  Trip Total  Payment Type                    Company  \\\n",
       "0    35.0       38.75   Credit Card               City Service   \n",
       "1     0.0       12.00          Cash  Taxi Affiliation Services   \n",
       "2     5.0       26.70   Credit Card               City Service   \n",
       "3     4.0       47.10   Credit Card                   Sun Taxi   \n",
       "4     0.0       22.82        Mobile                5 Star Taxi   \n",
       "\n",
       "   Pickup Centroid Latitude Pickup Centroid Longitude  \\\n",
       "0                 41.899602                -87.633308   \n",
       "1                 41.901207                -87.676356   \n",
       "2                 41.792592                -87.769615   \n",
       "3                 41.980264                -87.913625   \n",
       "4                 41.899602                -87.633308   \n",
       "\n",
       "               Pickup Centroid Location  Dropoff Centroid Latitude  \\\n",
       "0   POINT (-87.6333080367 41.899602111)                  41.899602   \n",
       "1  POINT (-87.6763559892 41.9012069941)                  41.899602   \n",
       "2  POINT (-87.7696154528 41.7925923603)                        NaN   \n",
       "3   POINT (-87.913624596 41.9802643146)                  41.965812   \n",
       "4   POINT (-87.6333080367 41.899602111)                  41.965812   \n",
       "\n",
       "   Dropoff Centroid Longitude           Dropoff Centroid  Location  \n",
       "0                  -87.633308  POINT (-87.6333080367 41.899602111)  \n",
       "1                  -87.633308  POINT (-87.6333080367 41.899602111)  \n",
       "2                         NaN                                  NaN  \n",
       "3                  -87.655879   POINT (-87.6558787862 41.96581197)  \n",
       "4                  -87.655879   POINT (-87.6558787862 41.96581197)  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to your imports section\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "# Define a central configuration section with mappings\n",
    "def create_column_mapping():\n",
    "    \"\"\"Create a configurable mapping between dataset columns and database fields\"\"\"\n",
    "    \n",
    "    # Default mapping for typical taxi/rideshare datasets\n",
    "    default_mapping = {\n",
    "        # Source dataset column : Database column\n",
    "        \"start_time_local\": \"departure_time\",\n",
    "        \"end_time_local\": \"estimated_arrival_time\",\n",
    "        \"pickup_location_latitude\": \"source_latitude\",\n",
    "        \"pickup_location_longitude\": \"source_longitude\",\n",
    "        \"dropoff_location_latitude\": \"destination_latitude\",\n",
    "        \"dropoff_location_longitude\": \"destination_longitude\",\n",
    "        \"trip_distance_meters\": \"trip_distance_meters\"\n",
    "    }\n",
    "    \n",
    "    # You could also load mappings from a config file\n",
    "    mapping_file = os.getenv('COLUMN_MAPPING_FILE')\n",
    "    \n",
    "    if mapping_file and os.path.exists(mapping_file):\n",
    "        try:\n",
    "            with open(mapping_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading column mapping from {mapping_file}: {e}\")\n",
    "            print(\"Using default mapping instead.\")\n",
    "    \n",
    "    return default_mapping\n",
    "\n",
    "# Dataset-specific mappings\n",
    "DATASET_MAPPINGS = {\n",
    "    \"san_francisco\": {\n",
    "        \"start_time_local\": \"departure_time\",\n",
    "        \"end_time_local\": \"estimated_arrival_time\",\n",
    "        \"pickup_location_latitude\": \"source_latitude\",\n",
    "        \"pickup_location_longitude\": \"source_longitude\",\n",
    "        \"dropoff_location_latitude\": \"destination_latitude\",\n",
    "        \"dropoff_location_longitude\": \"destination_longitude\",\n",
    "        \"trip_distance_meters\": \"trip_distance_meters\"\n",
    "    },\n",
    "    \"new_york\": {\n",
    "        \"pickup_datetime\": \"departure_time\",\n",
    "        \"dropoff_datetime\": \"estimated_arrival_time\",\n",
    "        \"pickup_latitude\": \"source_latitude\",\n",
    "        \"pickup_longitude\": \"source_longitude\",\n",
    "        \"dropoff_latitude\": \"destination_latitude\", \n",
    "        \"dropoff_longitude\": \"destination_longitude\",\n",
    "        \"trip_distance\": \"trip_distance_meters\"  # Might need conversion if not in meters\n",
    "    },\n",
    "    # Add more dataset mappings as needed\n",
    "}\n",
    "\n",
    "def load_dataset(file_path, dataset_type=None):\n",
    "    \"\"\"\n",
    "    Load and map a dataset from CSV file using configurable column mappings\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the dataset file\n",
    "        dataset_type: Optional dataset type identifier to use specific mapping\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get appropriate mapping for this dataset\n",
    "    if dataset_type and dataset_type in DATASET_MAPPINGS:\n",
    "        mapping = DATASET_MAPPINGS[dataset_type]\n",
    "    else:\n",
    "        # Try to infer the dataset type from the filename\n",
    "        filename = os.path.basename(file_path).lower()\n",
    "        \n",
    "        if \"francisco\" in filename or \"sf\" in filename:\n",
    "            mapping = DATASET_MAPPINGS[\"san_francisco\"]\n",
    "        elif \"york\" in filename or \"nyc\" in filename:\n",
    "            mapping = DATASET_MAPPINGS[\"new_york\"]\n",
    "        else:\n",
    "            # Use the default mapping\n",
    "            mapping = create_column_mapping()\n",
    "    \n",
    "    # Verify all required columns are present\n",
    "    missing_columns = [col for col in mapping.keys() if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns in dataset: {missing_columns}\")\n",
    "    \n",
    "    # Create a new DataFrame with mapped column names\n",
    "    mapped_df = pd.DataFrame()\n",
    "    \n",
    "    for source_col, target_col in mapping.items():\n",
    "        mapped_df[target_col] = df[source_col]\n",
    "    \n",
    "    # Handle special conversions if needed\n",
    "    # For example, if distance is in miles instead of meters\n",
    "    if dataset_type == \"new_york\" and \"trip_distance_meters\" in mapped_df:\n",
    "        # Convert miles to meters (1 mile â‰ˆ 1609 meters)\n",
    "        mapped_df[\"trip_distance_meters\"] = df[\"trip_distance\"] * 1609.34\n",
    "    \n",
    "    return mapped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08688bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"Load and filter a dataset from CSV file\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    columns_to_keep = [\n",
    "        'start_time_local',\n",
    "        'end_time_local',\n",
    "        'pickup_location_latitude',\n",
    "        'pickup_location_longitude',\n",
    "        'dropoff_location_latitude',\n",
    "        'dropoff_location_longitude',\n",
    "        'trip_distance_meters'\n",
    "    ]\n",
    "    \n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def connect_to_database():\n",
    "    \"\"\"Connect to the database using environment variables\"\"\"\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Database connection parameters\n",
    "    db_params = {\n",
    "        'host': os.getenv('DB_HOST', 'localhost'),\n",
    "        'database': os.getenv('DB_NAME', 'matching_engine'),\n",
    "        'user': os.getenv('DB_USER', 'postgres'),\n",
    "        'password': os.getenv('DB_PASSWORD'),\n",
    "        'port': os.getenv('DB_PORT', '5432')\n",
    "    }\n",
    "    \n",
    "    # Check if password is available\n",
    "    if not db_params['password']:\n",
    "        raise ValueError(\"DB_PASSWORD environment variable is not set\")\n",
    "    \n",
    "    # Connect to database and return connection and cursor\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    return conn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da159ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meters_to_minutes(meters):\n",
    "    \"\"\"Convert trip distance in meters to estimated duration in minutes\"\"\"\n",
    "    # Assuming average speed of 30 km/h = 500 meters per minute\n",
    "    return max(1, int(meters / 500))\n",
    "\n",
    "def generate_detour_duration_minutes(weights):\n",
    "    values = [0, 10, 15, 20, 25, 30]\n",
    "    return random.choices(values, weights=weights)[0]\n",
    "\n",
    "def generate_capacity():\n",
    "    values = [1, 2, 3]\n",
    "    weights = [0.1, 0.4, 0.35, 0.15]\n",
    "    return random.choices(values, weights=weights)[0]\n",
    "\n",
    "def generate_max_walking_duration_minutes():\n",
    "    values = [5, 10, 15, 20, 25]\n",
    "    weights = [0.3, 0.35, 0.2, 0.1, 0.05]\n",
    "    return random.choices(values, weights=weights)[0]\n",
    "\n",
    "def generate_number_of_riders_per_request():\n",
    "    values = [1, 2, 3]\n",
    "    weights = [0.6, 0.3, 0.1]\n",
    "    return random.choices(values, weights=weights)[0]\n",
    "\n",
    "def generate_same_gender_preference(gender):\n",
    "    weights = {\n",
    "        'female': [0.65, 0.35],\n",
    "        'male': [0.15, 0.85]   \n",
    "    }\n",
    "    gender_weights = weights.get(gender, [0.3, 0.7])  \n",
    "    \n",
    "    return random.choices([True, False], weights=gender_weights)[0]\n",
    "\n",
    "def generate_gender(driver=False):\n",
    "    if driver:\n",
    "        return random.choices(['male', 'female'], weights=[0.7, 0.3])[0]\n",
    "    else:\n",
    "        return random.choices(['male', 'female'], weights=[0.45, 0.55])[0]\n",
    "\n",
    "def generate_max_estimated_arrival_time(distance, detour_duration):\n",
    "    \"\"\"Generate a random maximum estimated arrival time\"\"\"\n",
    "    return meters_to_minutes(distance) + detour_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rider_requests(data_chunk, start_idx, dataset_id=None, config=None):\n",
    "    \"\"\"Create rider requests from a data chunk\"\"\"\n",
    "    requests = []\n",
    "    \n",
    "    for i, row in data_chunk.iterrows():\n",
    "        # Generate a unique ID with optional dataset prefix\n",
    "        prefix = f\"ds{dataset_id}-\" if dataset_id is not None else \"\"\n",
    "        request_id = f\"{prefix}req-{start_idx + i:06d}\"\n",
    "        user_id = f\"{prefix}user-{random.randint(1000, 9999)}\"\n",
    "        \n",
    "        # Parse timestamps\n",
    "        earliest_departure = pd.to_datetime(row['start_time_local'])\n",
    "        latest_arrival = pd.to_datetime(row['end_time_local'])\n",
    "        \n",
    "        # First generate gender\n",
    "        gender = generate_gender(driver=False)\n",
    "        \n",
    "        # Then generate preferences that correlate with gender\n",
    "        prefs = generate_preferences(gender)\n",
    "        \n",
    "        # Generate other parameters with appropriate configurations\n",
    "        num_riders = generate_capacity(is_driver=False)\n",
    "        max_walking = generate_max_walking_duration_minutes()\n",
    "        \n",
    "        requests.append((\n",
    "            request_id,\n",
    "            user_id,\n",
    "            float(row['pickup_location_latitude']),\n",
    "            float(row['pickup_location_longitude']),\n",
    "            None,  # source_address\n",
    "            float(row['dropoff_location_latitude']),\n",
    "            float(row['dropoff_location_longitude']),\n",
    "            None,  # destination_address\n",
    "            earliest_departure,\n",
    "            latest_arrival,\n",
    "            max_walking,\n",
    "            num_riders,\n",
    "            prefs['same_gender'],\n",
    "            prefs['allows_smoking'],\n",
    "            prefs['allows_pets'],\n",
    "            gender,\n",
    "            dataset_id  # Add dataset_id as the last column\n",
    "        ))\n",
    "    \n",
    "    return requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ed61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_driver_offers(data_chunk, start_idx, dataset_id=None):\n",
    "    offers = []\n",
    "    \n",
    "    for i, row in data_chunk.iterrows():\n",
    "        # Generate a unique ID\n",
    "        prefix = f\"ds{dataset_id}-\" if dataset_id is not None else \"\"\n",
    "        offer_id = f\"{prefix}drv-{start_idx + i:06d}\"\n",
    "        user_id = f\"{prefix}user-{random.randint(10000, 99999)}\"\n",
    "        \n",
    "        # Parse timestamps\n",
    "        departure_time = pd.to_datetime(row['start_time_local'])\n",
    "        estimated_arrival = pd.to_datetime(row['end_time_local'])\n",
    "        \n",
    "        # Calculate maximum estimated arrival (add buffer)\n",
    "        buffer_minutes = random.randint(5, 30)\n",
    "        max_arrival = estimated_arrival + timedelta(minutes=buffer_minutes)\n",
    "        \n",
    "        # Convert distance to detour minutes\n",
    "        detour_minutes = meters_to_minutes(row['trip_distance_meters'])\n",
    "        \n",
    "        # Random capacity (2-6)\n",
    "        capacity = random.randint(2, 6)\n",
    "        \n",
    "        # Generate random preferences\n",
    "        prefs = random_preferences()\n",
    "        gender = random_gender()\n",
    "        \n",
    "        offers.append((\n",
    "            offer_id,\n",
    "            user_id,\n",
    "            float(row['pickup_location_latitude']),\n",
    "            float(row['pickup_location_longitude']),\n",
    "            None,  # source_address\n",
    "            float(row['dropoff_location_latitude']),\n",
    "            float(row['dropoff_location_longitude']),\n",
    "            None,  # destination_address\n",
    "            departure_time,\n",
    "            max_arrival,\n",
    "            estimated_arrival,\n",
    "            detour_minutes,\n",
    "            capacity,\n",
    "            0,  # current_number_of_requests\n",
    "            prefs['same_gender'],\n",
    "            prefs['allows_smoking'],\n",
    "            prefs['allows_pets'],\n",
    "            gender,\n",
    "            dataset_id  # Add dataset_id as the last column\n",
    "        ))\n",
    "    \n",
    "    return offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca5c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_database(df, chunk_size=1000, dataset_id=None, rider_ratio=0.6):\n",
    "    \"\"\"Process and insert data into database tables\"\"\"\n",
    "    conn = connect_to_database()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Split data: rider_ratio% for rider requests, the rest for driver offers\n",
    "        split_point = int(len(df) * rider_ratio)\n",
    "        rider_data = df.iloc[:split_point].reset_index(drop=True)\n",
    "        driver_data = df.iloc[split_point:].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Creating {len(rider_data)} rider requests and {len(driver_data)} driver offers\")\n",
    "        \n",
    "        # Insert rider requests\n",
    "        for chunk_start in range(0, len(rider_data), chunk_size):\n",
    "            chunk_end = min(chunk_start + chunk_size, len(rider_data))\n",
    "            chunk = rider_data.iloc[chunk_start:chunk_end]\n",
    "            \n",
    "            requests = create_rider_requests(chunk, chunk_start, dataset_id)\n",
    "            \n",
    "            # SQL for inserting rider requests\n",
    "            insert_rider_sql = \"\"\"\n",
    "            INSERT INTO rider_requests (\n",
    "                id, user_id, \n",
    "                source_latitude, source_longitude, source_address,\n",
    "                destination_latitude, destination_longitude, destination_address,\n",
    "                earliest_departure_time, latest_arrival_time,\n",
    "                max_walking_duration_minutes, number_of_riders,\n",
    "                same_gender, allows_smoking, allows_pets, user_gender, dataset_id\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            \n",
    "            execute_batch(cursor, insert_rider_sql, requests)\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {len(requests)} rider requests\")\n",
    "        \n",
    "        # Insert driver offers\n",
    "        for chunk_start in range(0, len(driver_data), chunk_size):\n",
    "            chunk_end = min(chunk_start + chunk_size, len(driver_data))\n",
    "            chunk = driver_data.iloc[chunk_start:chunk_end]\n",
    "            \n",
    "            offers = create_driver_offers(chunk, chunk_start, dataset_id)\n",
    "            \n",
    "            # SQL for inserting driver offers\n",
    "            # SQL for inserting driver offers\n",
    "            insert_driver_sql = \"\"\"\n",
    "            INSERT INTO driver_offers (\n",
    "                id, user_id,\n",
    "                source_latitude, source_longitude, source_address,\n",
    "                destination_latitude, destination_longitude, destination_address,\n",
    "                departure_time, max_estimated_arrival_time, estimated_arrival_time,\n",
    "                detour_duration_minutes, capacity, current_number_of_requests,\n",
    "                same_gender, allows_smoking, allows_pets, user_gender, dataset_id\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            \n",
    "            execute_batch(cursor, insert_driver_sql, offers)\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {len(offers)} driver offers\")\n",
    "        \n",
    "        print(\"Data import completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2555a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiple_datasets(file_path, dataset_count=3, rows_per_dataset=None):\n",
    "    \"\"\"Create multiple distinct datasets in the database\"\"\"\n",
    "    df = load_dataset(file_path)\n",
    "    \n",
    "    # If rows_per_dataset is not specified, use the full dataset for each\n",
    "    if rows_per_dataset is None or rows_per_dataset > len(df):\n",
    "        rows_per_dataset = len(df)\n",
    "    \n",
    "    for dataset_id in range(1, dataset_count + 1):\n",
    "        print(f\"\\nCreating dataset #{dataset_id}\")\n",
    "        \n",
    "        # Sample subset of data\n",
    "        if len(df) > rows_per_dataset:\n",
    "            sample_df = df.sample(n=rows_per_dataset)\n",
    "        else:\n",
    "            sample_df = df\n",
    "            \n",
    "        # Apply time shift for dataset variety (optional)\n",
    "        # Each dataset shifts forward by a week from the last\n",
    "        day_shift = timedelta(days=(dataset_id-1) * 7)\n",
    "        sample_df = sample_df.copy()\n",
    "        sample_df['start_time_local'] = pd.to_datetime(sample_df['start_time_local']) + day_shift\n",
    "        sample_df['end_time_local'] = pd.to_datetime(sample_df['end_time_local']) + day_shift\n",
    "        \n",
    "        # Insert this dataset\n",
    "        insert_into_database(sample_df, dataset_id=dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91107bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create a single dataset\n",
    "    # df = load_dataset(\"C:/Users/mariam/Downloads/Datasets/New folder/san francisco/Taxi_Trips.csv\")\n",
    "    # insert_into_database(df)\n",
    "    \n",
    "    # Or create multiple datasets\n",
    "    create_multiple_datasets(\n",
    "        \"C:/Users/mariam/Downloads/Datasets/New folder/san francisco/Taxi_Trips.csv\",\n",
    "        dataset_count=3,\n",
    "        rows_per_dataset=5000  # Number of rows per dataset\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
